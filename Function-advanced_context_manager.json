[{"id":"advanced_context_manager","user_id":"4a8b3bd3-9c35-4347-a932-1c60a2bc0fac","name":"Advanced Context Manager","type":"filter","content":"\"\"\"\ntitle: Advanced Context Manager - Recursive Summary Without Truncation\nauthor: JiangNanGenius\nGithub: https://github.com/JiangNanGenius/Advanced-Context-Manager/\nversion: 7.0.0\nlicense: MIT\nrequired_open_webui_version: 0.4.0\n\"\"\"\n\nimport json\nimport hashlib\nimport asyncio\nfrom typing import Optional, List, Dict, Callable, Any, Tuple\nfrom pydantic import BaseModel, Field\n\n# 导入所需库\ntry:\n    import tiktoken\n\n    TIKTOKEN_AVAILABLE = True\nexcept ImportError:\n    TIKTOKEN_AVAILABLE = False\n    tiktoken = None\n\ntry:\n    from openai import AsyncOpenAI\n\n    OPENAI_AVAILABLE = True\nexcept ImportError:\n    OPENAI_AVAILABLE = False\n    AsyncOpenAI = None\n\n\nclass Filter:\n    class Valves(BaseModel):\n        enable_auto_summary: bool = Field(\n            default=True, description=\"启用自动对话摘要功能\"\n        )\n\n        # Debug设置\n        debug_level: int = Field(\n            default=1, description=\"调试级别：0=关闭，1=基础，2=详细，3=完整\"\n        )\n\n        # Token管理\n        total_token_limit: int = Field(\n            default=40000, description=\"总token限制（建议设置为模型限制的60-70%）\"\n        )\n\n        # 上下文保留策略 - 核心配置\n        context_preserve_ratio: float = Field(\n            default=0.7, description=\"上下文保留比例（0.7表示保留70%原文，30%总结）\"\n        )\n\n        large_message_threshold: int = Field(\n            default=10000,\n            description=\"大消息阈值，超过此值的消息将应用智能内部保留策略\",\n        )\n\n        max_recursion_depth: int = Field(\n            default=3, description=\"最大递归总结深度，防止无限循环\"\n        )\n\n        # 摘要配置\n        max_summary_length: int = Field(default=3000, description=\"摘要的最大token长度\")\n\n        # 摘要模型配置\n        summarizer_base_url: str = Field(\n            default=\"https://ark.cn-beijing.volces.com/api/v3\",\n            description=\"摘要模型的API基础URL\",\n        )\n\n        summarizer_api_key: str = Field(default=\"\", description=\"摘要模型的API密钥\")\n\n        summarizer_model: str = Field(\n            default=\"doubao-1-5-thinking-pro-250415\", description=\"用于摘要的模型名称\"\n        )\n\n        # 分片和并发配置\n        max_chunk_tokens: int = Field(\n            default=8000, description=\"每个分片的最大token数（摘要时）\"\n        )\n\n        max_concurrent_requests: int = Field(default=3, description=\"最大并发请求数\")\n\n        max_summary_tokens_per_chunk: int = Field(\n            default=500, description=\"每个分片摘要的最大token数\"\n        )\n\n        summarizer_temperature: float = Field(\n            default=0.2, description=\"摘要模型的温度参数\"\n        )\n\n        request_timeout: int = Field(default=60, description=\"API请求超时时间（秒）\")\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.conversation_summaries = {}\n        self._client = None\n        self._encoding = None\n\n    def debug_log(self, level: int, message: str):\n        \"\"\"分级debug日志\"\"\"\n        if self.valves.debug_level >= level:\n            prefix = [\"\", \"[DEBUG]\", \"[DETAIL]\", \"[VERBOSE]\"][min(level, 3)]\n            print(f\"{prefix} {message}\")\n\n    def get_encoding(self):\n        \"\"\"获取tiktoken编码器\"\"\"\n        if not TIKTOKEN_AVAILABLE:\n            return None\n        if self._encoding is None:\n            self._encoding = tiktoken.get_encoding(\"cl100k_base\")\n        return self._encoding\n\n    def count_tokens(self, text: str) -> int:\n        \"\"\"精确计算token数量\"\"\"\n        if not text:\n            return 0\n        encoding = self.get_encoding()\n        if encoding is None:\n            return len(text) // 4\n        try:\n            return len(encoding.encode(text))\n        except:\n            return len(text) // 4\n\n    def count_message_tokens(self, message: dict) -> int:\n        \"\"\"计算单个消息的token数\"\"\"\n        content = message.get(\"content\", \"\")\n        role = message.get(\"role\", \"\")\n        total_tokens = 0\n\n        if isinstance(content, list):\n            for item in content:\n                if item.get(\"type\") == \"text\":\n                    text = item.get(\"text\", \"\")\n                    total_tokens += self.count_tokens(text)\n        elif isinstance(content, str):\n            total_tokens = self.count_tokens(content)\n\n        total_tokens += self.count_tokens(role) + 4\n        return total_tokens\n\n    def count_messages_tokens(self, messages: List[dict]) -> int:\n        \"\"\"计算消息列表的总token数\"\"\"\n        return sum(self.count_message_tokens(msg) for msg in messages)\n\n    def analyze_messages(self, messages: List[dict], recursion_depth: int = 0) -> None:\n        \"\"\"详细分析消息token分布\"\"\"\n        depth_prefix = \"  \" * recursion_depth\n        self.debug_log(\n            2, f\"{depth_prefix}=== 消息分析 (递归深度: {recursion_depth}) ===\"\n        )\n        total = 0\n        user_total = 0\n        assistant_total = 0\n        system_total = 0\n\n        for i, msg in enumerate(messages):\n            role = msg.get(\"role\", \"unknown\")\n            content = msg.get(\"content\", \"\")\n            tokens = self.count_message_tokens(msg)\n            total += tokens\n\n            if role == \"user\":\n                user_total += tokens\n            elif role == \"assistant\":\n                assistant_total += tokens\n            elif role == \"system\":\n                system_total += tokens\n\n            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n            self.debug_log(\n                2,\n                f\"{depth_prefix}消息{i}: {role}, {tokens}tokens, 内容: {content_preview}\",\n            )\n\n        self.debug_log(\n            2,\n            f\"{depth_prefix}总计: {total}tokens (系统: {system_total}, 用户: {user_total}, 助手: {assistant_total})\",\n        )\n        self.debug_log(2, f\"{depth_prefix}=== 分析结束 ===\")\n\n    def smart_split_large_message(\n        self, message: dict, preserve_tokens: int\n    ) -> Tuple[dict, str]:\n        \"\"\"\n        智能分割大消息：保留70%原文，30%作为摘要内容\n        返回: (保留部分的消息, 需要摘要的内容)\n        \"\"\"\n        content = message.get(\"content\", \"\")\n        if not isinstance(content, str):\n            return message, \"\"\n\n        msg_tokens = self.count_message_tokens(message)\n        if msg_tokens <= preserve_tokens:\n            return message, \"\"\n\n        self.debug_log(\n            1, f\"智能分割大消息：{msg_tokens}tokens，保留{preserve_tokens}tokens原文\"\n        )\n\n        # 计算要保留的内容长度\n        encoding = self.get_encoding()\n\n        if encoding is None:\n            # 简单字符分割\n            preserve_ratio = preserve_tokens / msg_tokens\n            preserve_chars = int(len(content) * preserve_ratio)\n\n            # 从后面保留（通常最新的内容更重要）\n            if preserve_chars < len(content):\n                preserved_content = content[-preserve_chars:]\n                to_summarize_content = content[:-preserve_chars]\n            else:\n                preserved_content = content\n                to_summarize_content = \"\"\n        else:\n            # 精确token分割\n            tokens = encoding.encode(content)\n            preserve_token_count = preserve_tokens - 10  # 预留空间给role等\n\n            if preserve_token_count < len(tokens):\n                # 从后面保留\n                preserved_tokens = tokens[-preserve_token_count:]\n                to_summarize_tokens = tokens[:-preserve_token_count]\n\n                try:\n                    preserved_content = encoding.decode(preserved_tokens)\n                    to_summarize_content = encoding.decode(to_summarize_tokens)\n                except:\n                    # 如果解码失败，使用字符分割\n                    preserve_ratio = preserve_tokens / msg_tokens\n                    preserve_chars = int(len(content) * preserve_ratio)\n                    preserved_content = content[-preserve_chars:]\n                    to_summarize_content = content[:-preserve_chars]\n            else:\n                preserved_content = content\n                to_summarize_content = \"\"\n\n        # 创建保留的消息\n        preserved_message = message.copy()\n        preserved_message[\"content\"] = preserved_content\n\n        preserved_tokens = self.count_message_tokens(preserved_message)\n        to_summarize_tokens = self.count_tokens(to_summarize_content)\n\n        self.debug_log(\n            1,\n            f\"大消息分割完成：保留{preserved_tokens}tokens，待摘要{to_summarize_tokens}tokens\",\n        )\n\n        return preserved_message, to_summarize_content\n\n    def get_openai_client(self):\n        \"\"\"获取OpenAI客户端\"\"\"\n        if not OPENAI_AVAILABLE:\n            raise Exception(\"OpenAI库未安装，请执行: pip install openai\")\n        if not self.valves.summarizer_api_key:\n            raise Exception(\"未配置摘要API密钥\")\n\n        if self._client is None:\n            self._client = AsyncOpenAI(\n                base_url=self.valves.summarizer_base_url,\n                api_key=self.valves.summarizer_api_key,\n                timeout=self.valves.request_timeout,\n            )\n        return self._client\n\n    def extract_new_session(\n        self, messages: List[dict]\n    ) -> Tuple[List[dict], List[dict], List[dict]]:\n        \"\"\"\n        提取新会话（最后一条用户消息及其后的所有消息）\n        返回: (系统消息, 历史消息, 新会话消息)\n        \"\"\"\n        if not messages:\n            return [], [], []\n\n        # 分离系统消息\n        system_messages = [msg for msg in messages if msg.get(\"role\") == \"system\"]\n        conversation_messages = [msg for msg in messages if msg.get(\"role\") != \"system\"]\n\n        self.debug_log(\n            2,\n            f\"系统消息: {len(system_messages)}条, 对话消息: {len(conversation_messages)}条\",\n        )\n\n        if not conversation_messages:\n            return system_messages, [], []\n\n        # 找到最后一条用户消息的位置\n        last_user_idx = -1\n        for i in range(len(conversation_messages) - 1, -1, -1):\n            if conversation_messages[i].get(\"role\") == \"user\":\n                last_user_idx = i\n                break\n\n        if last_user_idx == -1:\n            # 没有用户消息，全部作为历史\n            self.debug_log(2, \"未找到用户消息，全部作为历史\")\n            return system_messages, conversation_messages, []\n\n        # 分割历史和新会话\n        history_messages = conversation_messages[:last_user_idx]\n        new_session_messages = conversation_messages[last_user_idx:]\n\n        self.debug_log(\n            2,\n            f\"新会话提取：历史{len(history_messages)}条，新会话{len(new_session_messages)}条\",\n        )\n\n        return system_messages, history_messages, new_session_messages\n\n    def smart_preserve_context_with_large_message_handling(\n        self, history_messages: List[dict], available_tokens: int\n    ) -> Tuple[List[dict], List[dict], List[str]]:\n        \"\"\"\n        智能保留上下文，包含大消息处理\n        返回: (保留的原文消息, 需要摘要的完整消息, 需要摘要的大消息片段)\n        \"\"\"\n        if not history_messages or available_tokens <= 0:\n            return [], history_messages, []\n\n        history_tokens = self.count_messages_tokens(history_messages)\n        self.debug_log(\n            1, f\"历史内容: {history_tokens}tokens, 可用空间: {available_tokens}tokens\"\n        )\n\n        if history_tokens <= available_tokens:\n            # 历史内容完全放得下，无需总结\n            self.debug_log(1, \"历史内容完全适合，无需总结\")\n            return history_messages, [], []\n\n        # 计算保留的token数\n        preserve_tokens = int(available_tokens * self.valves.context_preserve_ratio)\n        summary_budget = available_tokens - preserve_tokens\n\n        self.debug_log(\n            1,\n            f\"按{self.valves.context_preserve_ratio:.1%}比例保留：保留{preserve_tokens}tokens原文，{summary_budget}tokens用于总结\",\n        )\n\n        # 检查是否有大消息需要特殊处理\n        large_message_fragments = []\n        preserved_messages = []\n        to_summarize_messages = []\n        current_preserve_tokens = 0\n\n        # 从后往前处理消息\n        for i in range(len(history_messages) - 1, -1, -1):\n            msg = history_messages[i]\n            msg_tokens = self.count_message_tokens(msg)\n\n            # 检查是否是大消息\n            if (\n                msg_tokens > self.valves.large_message_threshold\n                and current_preserve_tokens < preserve_tokens\n            ):\n                # 对大消息进行智能分割\n                remaining_preserve_tokens = preserve_tokens - current_preserve_tokens\n                if remaining_preserve_tokens > 0:\n                    self.debug_log(\n                        1,\n                        f\"处理大消息({msg_tokens}tokens)，剩余保留空间: {remaining_preserve_tokens}tokens\",\n                    )\n\n                    preserved_part, to_summarize_part = self.smart_split_large_message(\n                        msg, remaining_preserve_tokens\n                    )\n\n                    if preserved_part:\n                        preserved_messages.insert(0, preserved_part)\n                        current_preserve_tokens += self.count_message_tokens(\n                            preserved_part\n                        )\n\n                    if to_summarize_part:\n                        large_message_fragments.append(to_summarize_part)\n\n                    # 将前面的消息加入摘要队列\n                    to_summarize_messages = history_messages[:i] + to_summarize_messages\n                    break\n                else:\n                    # 保留空间已满，全部摘要\n                    to_summarize_messages = (\n                        history_messages[: i + 1] + to_summarize_messages\n                    )\n                    break\n            else:\n                # 普通消息\n                if current_preserve_tokens + msg_tokens <= preserve_tokens:\n                    preserved_messages.insert(0, msg)\n                    current_preserve_tokens += msg_tokens\n                else:\n                    # 超出保留空间，剩余的全部摘要\n                    to_summarize_messages = history_messages[: i + 1]\n                    break\n\n        preserved_tokens = self.count_messages_tokens(preserved_messages)\n        to_summarize_tokens = self.count_messages_tokens(to_summarize_messages)\n        fragment_tokens = sum(\n            self.count_tokens(frag) for frag in large_message_fragments\n        )\n\n        self.debug_log(\n            1,\n            f\"智能保留结果：保留{len(preserved_messages)}条消息({preserved_tokens}tokens)，\"\n            + f\"摘要{len(to_summarize_messages)}条消息({to_summarize_tokens}tokens)，\"\n            + f\"大消息片段{len(large_message_fragments)}个({fragment_tokens}tokens)\",\n        )\n\n        return preserved_messages, to_summarize_messages, large_message_fragments\n\n    def split_large_message_for_summary(\n        self, message: dict, max_tokens: int\n    ) -> List[str]:\n        \"\"\"\n        将过大的消息分割成多个片段用于摘要（保持完整内容）\n        返回: 分割后的内容片段列表\n        \"\"\"\n        content = message.get(\"content\", \"\")\n        if not isinstance(content, str):\n            return [content]\n\n        tokens = self.count_message_tokens(message)\n        if tokens <= max_tokens:\n            return [content]\n\n        self.debug_log(\n            2, f\"分割大消息用于摘要：{tokens}tokens → 多个{max_tokens}tokens片段\"\n        )\n\n        encoding = self.get_encoding()\n        if encoding is None:\n            # 简单字符分割\n            chunk_size = max_tokens * 3  # 粗略估算\n            chunks = []\n            for i in range(0, len(content), chunk_size):\n                chunks.append(content[i : i + chunk_size])\n            return chunks\n        else:\n            # 精确token分割\n            encoded_tokens = encoding.encode(content)\n            chunks = []\n\n            for i in range(0, len(encoded_tokens), max_tokens - 100):  # 预留空间\n                chunk_tokens = encoded_tokens[i : i + max_tokens - 100]\n                try:\n                    chunk_text = encoding.decode(chunk_tokens)\n                    chunks.append(chunk_text)\n                except:\n                    # 如果解码失败，使用字符分割\n                    start_pos = i * 3\n                    end_pos = min((i + max_tokens - 100) * 3, len(content))\n                    chunks.append(content[start_pos:end_pos])\n\n            return chunks\n\n    def create_summary_chunks_with_fragments(\n        self, messages: List[dict], fragments: List[str]\n    ) -> List[dict]:\n        \"\"\"创建用于摘要的chunks，包含大消息片段\"\"\"\n        chunks = []\n\n        # 处理完整消息\n        if messages:\n            self.debug_log(2, f\"处理{len(messages)}条完整消息用于摘要\")\n            current_chunk = []\n            current_tokens = 0\n            max_tokens = self.valves.max_chunk_tokens - 1000\n\n            for i, msg in enumerate(messages):\n                msg_tokens = self.count_message_tokens(msg)\n\n                if msg_tokens > max_tokens:\n                    # 大消息需要分割\n                    if current_chunk:\n                        chunks.append({\"type\": \"messages\", \"content\": current_chunk})\n                        current_chunk = []\n                        current_tokens = 0\n\n                    # 分割大消息\n                    content_chunks = self.split_large_message_for_summary(\n                        msg, max_tokens\n                    )\n                    role = msg.get(\"role\", \"unknown\")\n\n                    for content_chunk in content_chunks:\n                        chunks.append(\n                            {\n                                \"type\": \"message_fragment\",\n                                \"role\": role,\n                                \"content\": content_chunk,\n                            }\n                        )\n                else:\n                    if current_tokens + msg_tokens > max_tokens and current_chunk:\n                        chunks.append({\"type\": \"messages\", \"content\": current_chunk})\n                        current_chunk = [msg]\n                        current_tokens = msg_tokens\n                    else:\n                        current_chunk.append(msg)\n                        current_tokens += msg_tokens\n\n            if current_chunk:\n                chunks.append({\"type\": \"messages\", \"content\": current_chunk})\n\n        # 处理大消息片段\n        for fragment in fragments:\n            chunks.append({\"type\": \"large_fragment\", \"content\": fragment})\n\n        self.debug_log(1, f\"创建摘要chunks完成，共{len(chunks)}个片段\")\n        return chunks\n\n    async def update_progress(\n        self,\n        __event_emitter__,\n        current: int,\n        total: int,\n        stage: str,\n        recursion_depth: int = 0,\n    ):\n        \"\"\"更新进度显示\"\"\"\n        if __event_emitter__ and total > 0:\n            percentage = int((current / total) * 100)\n            depth_info = f\" (递归: {recursion_depth})\" if recursion_depth > 0 else \"\"\n            await self.send_status(\n                __event_emitter__,\n                f\"{stage}{depth_info} - 进度: {current}/{total} ({percentage}%)\",\n                False,\n            )\n\n    async def summarize_chunk_async_with_progress(\n        self,\n        chunk_data: dict,\n        chunk_index: int,\n        total_chunks: int,\n        summary_token_budget: int,\n        semaphore: asyncio.Semaphore,\n        __event_emitter__,\n        recursion_depth: int = 0,\n    ) -> Tuple[int, str]:\n        \"\"\"异步摘要单个chunk，包含进度更新\"\"\"\n        async with semaphore:\n            try:\n                # 更新进度\n                await self.update_progress(\n                    __event_emitter__,\n                    chunk_index + 1,\n                    total_chunks,\n                    \"正在摘要\",\n                    recursion_depth,\n                )\n\n                client = self.get_openai_client()\n\n                # 根据chunk类型构建对话文本\n                chunk_type = chunk_data.get(\"type\", \"messages\")\n\n                if chunk_type == \"messages\":\n                    # 处理完整消息列表\n                    conversation_text = \"\"\n                    for msg in chunk_data[\"content\"]:\n                        role = msg.get(\"role\", \"unknown\")\n                        content = str(msg.get(\"content\", \"\"))\n                        conversation_text += f\"{role}: {content}\\n\\n\"\n                    context_info = (\n                        f\"这是一段包含{len(chunk_data['content'])}条消息的对话\"\n                    )\n\n                elif chunk_type == \"message_fragment\":\n                    # 处理消息片段\n                    role = chunk_data.get(\"role\", \"unknown\")\n                    content = chunk_data.get(\"content\", \"\")\n                    conversation_text = f\"{role}: {content}\\n\\n\"\n                    context_info = f\"这是一个{role}消息的片段\"\n\n                elif chunk_type == \"large_fragment\":\n                    # 处理大消息片段\n                    content = chunk_data.get(\"content\", \"\")\n                    conversation_text = f\"content: {content}\\n\\n\"\n                    context_info = \"这是一个大消息的片段\"\n\n                else:\n                    conversation_text = str(chunk_data)\n                    context_info = \"未知类型的内容\"\n\n                # 计算这个chunk的摘要token限制\n                max_tokens_per_chunk = min(\n                    self.valves.max_summary_tokens_per_chunk,\n                    (\n                        max(300, summary_token_budget // total_chunks)\n                        if total_chunks > 0\n                        else 500\n                    ),\n                )\n\n                recursion_info = (\n                    f\"（递归深度: {recursion_depth}）\" if recursion_depth > 0 else \"\"\n                )\n\n                system_prompt = f\"\"\"你是专业的对话摘要专家。请为这段内容创建简洁但完整的摘要（第{chunk_index + 1}部分，共{total_chunks}部分）{recursion_info}。\n\n内容说明：{context_info}\n\n摘要要求：\n1. 保留所有重要信息、关键决定和讨论要点\n2. 如果涉及技术内容、数据或代码，务必保留核心信息\n3. 保持内容的逻辑流程和因果关系\n4. 如果内容很多，请分要点总结，确保不遗漏重要信息\n5. 摘要长度控制在{max_tokens_per_chunk}字以内\n6. 使用简洁准确的语言，但不能丢失重要细节\n\n内容：\"\"\"\n\n                user_prompt = f\"请根据要求摘要以下内容：\\n\\n{conversation_text}\"\n\n                response = await client.chat.completions.create(\n                    model=self.valves.summarizer_model,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_prompt},\n                        {\"role\": \"user\", \"content\": user_prompt},\n                    ],\n                    max_tokens=max_tokens_per_chunk,\n                    temperature=self.valves.summarizer_temperature,\n                    stream=False,\n                )\n\n                if response.choices and len(response.choices) > 0:\n                    summary = response.choices[0].message.content.strip()\n                    self.debug_log(\n                        3, f\"Chunk {chunk_index + 1}摘要完成: {len(summary)}字符\"\n                    )\n                    return chunk_index, summary\n                else:\n                    return chunk_index, f\"第{chunk_index + 1}部分摘要失败（无响应）\"\n\n            except Exception as e:\n                self.debug_log(1, f\"摘要chunk {chunk_index + 1}时出错: {str(e)}\")\n                return (\n                    chunk_index,\n                    f\"第{chunk_index + 1}部分摘要失败: {str(e)[:100]}...\",\n                )\n            finally:\n                # 避免请求过于频繁\n                await asyncio.sleep(0.3)\n\n    async def create_comprehensive_summary_with_progress(\n        self,\n        messages_to_summarize: List[dict],\n        large_fragments: List[str],\n        summary_token_budget: int,\n        __event_emitter__,\n        recursion_depth: int = 0,\n    ) -> str:\n        \"\"\"创建comprehensive摘要，包含进度显示\"\"\"\n        if not messages_to_summarize and not large_fragments:\n            return \"\"\n\n        # 创建摘要chunks\n        chunks = self.create_summary_chunks_with_fragments(\n            messages_to_summarize, large_fragments\n        )\n        if not chunks:\n            return \"\"\n\n        original_tokens = self.count_messages_tokens(messages_to_summarize)\n        fragment_tokens = sum(self.count_tokens(frag) for frag in large_fragments)\n        total_original_tokens = original_tokens + fragment_tokens\n\n        depth_info = f\" (递归: {recursion_depth})\" if recursion_depth > 0 else \"\"\n        await self.send_status(\n            __event_emitter__,\n            f\"开始摘要 {len(chunks)} 个片段（原始{total_original_tokens}tokens，保留{self.valves.context_preserve_ratio:.1%}原文）{depth_info}\",\n            False,\n        )\n\n        # 并发摘要处理\n        semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)\n        tasks = []\n\n        for i, chunk in enumerate(chunks):\n            task = self.summarize_chunk_async_with_progress(\n                chunk,\n                i,\n                len(chunks),\n                summary_token_budget,\n                semaphore,\n                __event_emitter__,\n                recursion_depth,\n            )\n            tasks.append(task)\n\n        try:\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n        except Exception as e:\n            await self.send_status(__event_emitter__, f\"摘要处理出错: {str(e)}\")\n            return \"摘要处理失败\"\n\n        # 处理结果\n        summaries = {}\n        successful_count = 0\n\n        for result in results:\n            if isinstance(result, Exception):\n                self.debug_log(1, f\"摘要任务异常: {result}\")\n                continue\n\n            chunk_index, summary = result\n            summaries[chunk_index] = summary\n            successful_count += 1\n\n        await self.update_progress(\n            __event_emitter__,\n            successful_count,\n            len(chunks),\n            \"摘要完成\",\n            recursion_depth,\n        )\n        self.debug_log(1, f\"摘要完成：{successful_count}/{len(chunks)}个片段成功\")\n\n        # 按顺序组合摘要\n        ordered_summaries = []\n        for i in range(len(chunks)):\n            if i in summaries:\n                ordered_summaries.append(summaries[i])\n\n        if not ordered_summaries:\n            return \"所有摘要任务都失败了\"\n\n        # 组合最终摘要\n        if len(ordered_summaries) == 1:\n            final_summary = ordered_summaries[0]\n        else:\n            final_summary = \"\\n\\n\".join(\n                f\"【第{i+1}部分摘要】\\n{summary}\"\n                for i, summary in enumerate(ordered_summaries)\n            )\n\n        summary_tokens = self.count_tokens(final_summary)\n        self.debug_log(\n            1,\n            f\"最终摘要：{summary_tokens}tokens，原始内容：{total_original_tokens}tokens\",\n        )\n\n        return final_summary\n\n    async def recursive_context_processing(\n        self, messages: List[dict], __event_emitter__, recursion_depth: int = 0\n    ) -> List[dict]:\n        \"\"\"\n        递归上下文处理：如果结果超限则再次执行总结流程\n        返回: 最终处理后的消息列表\n        \"\"\"\n        if recursion_depth >= self.valves.max_recursion_depth:\n            self.debug_log(\n                1, f\"达到最大递归深度 {self.valves.max_recursion_depth}，停止处理\"\n            )\n            await self.send_status(__event_emitter__, f\"达到最大递归深度，停止处理\")\n            return messages\n\n        if not messages:\n            return messages\n\n        total_tokens = self.count_messages_tokens(messages)\n        self.debug_log(\n            1,\n            f\"递归处理 (深度: {recursion_depth})：{total_tokens}tokens，限制: {self.valves.total_token_limit}\",\n        )\n\n        # 详细分析消息\n        if self.valves.debug_level >= 2:\n            self.analyze_messages(messages, recursion_depth)\n\n        # 检查是否需要处理\n        if total_tokens <= self.valves.total_token_limit:\n            self.debug_log(1, f\"递归深度 {recursion_depth}：内容未超限，处理完成\")\n            if recursion_depth > 0:\n                await self.send_status(\n                    __event_emitter__, f\"递归总结完成 (深度: {recursion_depth})\"\n                )\n            return messages\n\n        # 超限处理\n        if recursion_depth > 0:\n            await self.send_status(\n                __event_emitter__,\n                f\"第{recursion_depth}次递归：内容仍超限({total_tokens}>{self.valves.total_token_limit})，继续总结...\",\n                False,\n            )\n\n        # 提取新会话\n        system_messages, history_messages, new_session_messages = (\n            self.extract_new_session(messages)\n        )\n\n        # 计算token分布\n        system_tokens = self.count_messages_tokens(system_messages)\n        history_tokens = self.count_messages_tokens(history_messages)\n        new_session_tokens = self.count_messages_tokens(new_session_messages)\n\n        self.debug_log(\n            1,\n            f\"递归 {recursion_depth} Token分布 - 系统: {system_tokens}, 历史: {history_tokens}, 新会话: {new_session_tokens}\",\n        )\n\n        # 计算历史内容可用空间\n        available_for_history = (\n            self.valves.total_token_limit - system_tokens - new_session_tokens\n        )\n\n        if available_for_history <= 0:\n            self.debug_log(\n                1, f\"递归 {recursion_depth}：新会话已占满所有空间，历史内容将全部摘要\"\n            )\n            summary_budget = min(\n                self.valves.max_summary_length, self.valves.total_token_limit // 10\n            )\n            final_messages = system_messages + new_session_messages\n\n            if history_messages:\n                # 对历史内容进行摘要\n                summary = await self.create_comprehensive_summary_with_progress(\n                    history_messages,\n                    [],\n                    summary_budget,\n                    __event_emitter__,\n                    recursion_depth,\n                )\n\n                if summary and summary.strip():\n                    summary_content = (\n                        f\"=== 历史对话摘要 (递归: {recursion_depth}) ===\\n{summary}\"\n                    )\n\n                    # 添加摘要到系统消息\n                    system_msg_found = False\n                    for msg in final_messages:\n                        if msg.get(\"role\") == \"system\":\n                            msg[\"content\"] = f\"{msg['content']}\\n\\n{summary_content}\"\n                            system_msg_found = True\n                            break\n\n                    if not system_msg_found:\n                        final_messages.insert(\n                            0, {\"role\": \"system\", \"content\": summary_content}\n                        )\n\n            # 检查结果是否还超限\n            result_tokens = self.count_messages_tokens(final_messages)\n            if result_tokens > self.valves.total_token_limit:\n                # 递归处理\n                return await self.recursive_context_processing(\n                    final_messages, __event_emitter__, recursion_depth + 1\n                )\n            else:\n                return final_messages\n\n        self.debug_log(\n            1,\n            f\"递归 {recursion_depth}：历史内容可用空间: {available_for_history}tokens\",\n        )\n\n        # 智能保留历史内容\n        preserved_history, to_summarize, large_fragments = (\n            self.smart_preserve_context_with_large_message_handling(\n                history_messages, available_for_history\n            )\n        )\n\n        # 组装结果\n        final_messages = system_messages + preserved_history + new_session_messages\n\n        if to_summarize or large_fragments:\n            # 有内容需要摘要\n            preserved_tokens = self.count_messages_tokens(preserved_history)\n            summary_budget = available_for_history - preserved_tokens\n            summary_budget = min(summary_budget, self.valves.max_summary_length)\n\n            total_items = len(to_summarize) + len(large_fragments)\n            await self.send_status(\n                __event_emitter__,\n                f\"递归 {recursion_depth}：开始摘要 {total_items} 项内容，保留 {self.valves.context_preserve_ratio:.1%} 原文...\",\n                False,\n            )\n\n            summary = await self.create_comprehensive_summary_with_progress(\n                to_summarize,\n                large_fragments,\n                summary_budget,\n                __event_emitter__,\n                recursion_depth,\n            )\n\n            if summary and summary.strip() and summary != \"所有摘要任务都失败了\":\n                # 将摘要添加到系统消息\n                to_summarize_tokens = self.count_messages_tokens(to_summarize)\n                fragment_tokens = sum(\n                    self.count_tokens(frag) for frag in large_fragments\n                )\n                total_summarized_tokens = to_summarize_tokens + fragment_tokens\n\n                summary_content = f\"=== 历史对话摘要 (递归: {recursion_depth}, {len(to_summarize)}条消息+{len(large_fragments)}个片段，{total_summarized_tokens}tokens原始内容，保留{self.valves.context_preserve_ratio:.1%}原文) ===\\n{summary}\"\n\n                # 添加摘要到系统消息\n                system_msg_found = False\n                for msg in final_messages:\n                    if msg.get(\"role\") == \"system\":\n                        if (\n                            f\"=== 历史对话摘要 (递归: {recursion_depth}\"\n                            not in msg[\"content\"]\n                        ):\n                            msg[\"content\"] = f\"{msg['content']}\\n\\n{summary_content}\"\n                        system_msg_found = True\n                        break\n\n                if not system_msg_found:\n                    final_messages.insert(\n                        0, {\"role\": \"system\", \"content\": summary_content}\n                    )\n\n        # 检查最终结果是否还超限\n        result_tokens = self.count_messages_tokens(final_messages)\n        self.debug_log(\n            1,\n            f\"递归 {recursion_depth} 处理后: {len(final_messages)} 条消息, {result_tokens} tokens\",\n        )\n\n        if result_tokens > self.valves.total_token_limit:\n            self.debug_log(1, f\"递归 {recursion_depth} 结果仍超限，继续下一轮处理\")\n            # 递归处理\n            return await self.recursive_context_processing(\n                final_messages, __event_emitter__, recursion_depth + 1\n            )\n        else:\n            self.debug_log(1, f\"递归 {recursion_depth} 处理成功，结果在限制内\")\n            return final_messages\n\n    def get_chat_id(self, __event_emitter__) -> Optional[str]:\n        \"\"\"提取聊天ID\"\"\"\n        try:\n            if (\n                hasattr(__event_emitter__, \"__closure__\")\n                and __event_emitter__.__closure__\n            ):\n                info = __event_emitter__.__closure__[0].cell_contents\n                return info.get(\"chat_id\")\n        except:\n            pass\n        return None\n\n    async def send_status(self, __event_emitter__, message: str, done: bool = True):\n        \"\"\"发送状态消息\"\"\"\n        if __event_emitter__:\n            try:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": message,\n                            \"done\": done,\n                        },\n                    }\n                )\n            except:\n                pass\n\n    async def inlet(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Optional[Callable] = None,\n    ) -> dict:\n        \"\"\"主要处理逻辑\"\"\"\n        if not self.valves.enable_auto_summary:\n            return body\n\n        messages = body.get(\"messages\", [])\n        if not messages:\n            return body\n\n        # 检查依赖\n        if not OPENAI_AVAILABLE:\n            await self.send_status(__event_emitter__, \"错误：需要安装OpenAI库\")\n            return body\n\n        if not self.valves.summarizer_api_key:\n            await self.send_status(__event_emitter__, \"错误：未配置摘要API密钥\")\n            return body\n\n        # 计算原始总token数\n        total_tokens = self.count_messages_tokens(messages)\n        self.debug_log(\n            1, f\"当前总token数: {total_tokens}, 限制: {self.valves.total_token_limit}\"\n        )\n\n        # 检查是否需要处理\n        if total_tokens <= self.valves.total_token_limit:\n            self.debug_log(1, \"内容未超限，无需处理\")\n            return body\n\n        # 超限处理 - 使用递归总结策略\n        chat_id = (\n            self.get_chat_id(__event_emitter__) or f\"chat_{hash(str(messages[:2]))}\"\n        )\n\n        await self.send_status(\n            __event_emitter__,\n            f\"内容超限({total_tokens}>{self.valves.total_token_limit})，启动递归总结策略...\",\n            False,\n        )\n\n        try:\n            # 递归上下文处理\n            final_messages = await self.recursive_context_processing(\n                messages, __event_emitter__\n            )\n\n            # 应用处理结果\n            body[\"messages\"] = final_messages\n            final_tokens = self.count_messages_tokens(final_messages)\n\n            await self.send_status(\n                __event_emitter__,\n                f\"递归总结完成：{len(messages)}→{len(final_messages)}条消息，{total_tokens}→{final_tokens}tokens\",\n            )\n\n            self.debug_log(\n                1, f\"最终结果: {len(final_messages)} 条消息, {final_tokens} tokens\"\n            )\n\n            # 保存摘要记录\n            if chat_id:\n                summary_msgs = [\n                    msg\n                    for msg in final_messages\n                    if msg.get(\"role\") == \"system\"\n                    and \"=== 历史对话摘要\" in msg.get(\"content\", \"\")\n                ]\n                if summary_msgs:\n                    self.conversation_summaries[chat_id] = summary_msgs[-1][\"content\"]\n\n        except Exception as e:\n            await self.send_status(__event_emitter__, f\"递归总结失败: {str(e)}\")\n            self.debug_log(1, f\"错误: {str(e)}\")\n            if self.valves.debug_level >= 2:\n                import traceback\n\n                traceback.print_exc()\n\n        return body\n\n    async def outlet(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Optional[Callable] = None,\n    ) -> dict:\n        \"\"\"输出处理\"\"\"\n        return body\n","meta":{"description":"自动检测上下文长度：使用简单的令牌估算方法 智能总结：当上下文超限时自动创建对话摘要 保留最近消息：保持对话的连续性 内存存储：使用简单的字典存储摘要","manifest":{"title":"Advanced Context Manager - Recursive Summary Without Truncation","author":"JiangNanGenius","Github":"https://github.com/JiangNanGenius/Advanced-Context-Manager/","version":"7.0.0","license":"MIT","required_open_webui_version":"0.4.0"}},"is_active":true,"is_global":true,"updated_at":1751711672,"created_at":1751696595}]